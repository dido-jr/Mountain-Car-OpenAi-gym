{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MOUNTAIN CAR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from abc import ABC, abstractmethod\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set to `True` if you want the agent learn from the environment, set to `False` if you want to test the knowledge aquired previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Training == True:\n",
    "    env = gym.make('MountainCar-v0')\n",
    "else:\n",
    "    env = gym.make('MountainCar-v0', render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretize the state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = [20, 20]\n",
    "num_actions = env.action_space.n\n",
    "state_space = [np.linspace(env.observation_space.low[0], env.observation_space.high[0], num_states[0]), np.linspace(env.observation_space.low[1], env.observation_space.high[1], num_states[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base class for all policies. The method <code>apply</code> enforces the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    # Ovveride this method to implement policy application\n",
    "    # Returns the action given the state\n",
    "    def apply(self, state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A concrete policy to initialize the search for an optimal policy. Implements an $\\epsilon$-greedy technique for action choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(BasePolicy):\n",
    "    def __init__(self, environment, Q_table, epsilon):\n",
    "        self.environment = environment\n",
    "        self.Q_table = Q_table\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def apply(self, state):\n",
    "        # Explore random action with probability epsilon\n",
    "        if Training and random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        #Choose the best action according to Q_table with probability 1-epsilon\n",
    "        #If all actions have the same Q-value then break ties randomly\n",
    "        else:\n",
    "            max_action_value = -1 * sys.float_info.max\n",
    "\n",
    "            for action in range(num_actions):\n",
    "                if self.Q_table[state[0]][state[1]][action] > max_action_value:\n",
    "                    max_action_value = self.Q_table[state[0]][state[1]][action]\n",
    "                    best_action = action\n",
    "            return best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA algorithm (on-policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each episode, the agent chooses an action based on the epsilon-greedy policy (or greedy policy), executes the action, receives a reward and a new state, and then updates the Q-table based on the difference between the expected and actual reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    def __init__(self, environment, gamma, alpha, epsilon, episodes):\n",
    "        self.environment = environment\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "        if Training:\n",
    "            self.Q_table = np.zeros((21,21, num_actions))\n",
    "        else:\n",
    "            self.Q_table = np.load('Q_table.npy')\n",
    "\n",
    "        self.policy = EpsilonGreedyPolicy(environment, self.Q_table, epsilon)\n",
    "\n",
    "        self.rewards = np.zeros(self.episodes//100)\n",
    "\n",
    "    #Return the indices of the bins to which each value in input array belongs.\n",
    "    def digitizeState(self, state):\n",
    "        new_state_p = np.digitize(state[0], state_space[0])\n",
    "        new_state_v = np.digitize(state[1], state_space[1])\n",
    "        return [new_state_p, new_state_v]\n",
    "\n",
    "\n",
    "    def apply(self):\n",
    "        i = 0\n",
    "        total_reward = 0\n",
    "        for episode in range(self.episodes):\n",
    "            self.environment.reset()\n",
    "            state,_ = self.environment.reset()\n",
    "            state = self.digitizeState(state)\n",
    "            action = self.policy.apply(state)\n",
    "            done = False\n",
    "            if episode % 1000 == 0:  \n",
    "                print(\"Episode: \", episode)   \n",
    "            while not done:\n",
    "                nextstate, reward, done, tr, info = self.environment.step(action)\n",
    "                nextstate = self.digitizeState(nextstate)\n",
    "                next_action = self.policy.apply(nextstate)\n",
    "                self.Q_table[state[0]][state[1]][action] += self.alpha * (reward + self.gamma * self.Q_table[nextstate[0]][nextstate[1]][next_action] - self.Q_table[state[0]][state[1]][action])\n",
    "                state = nextstate\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "            if episode % 100 == 0:\n",
    "                self.rewards[i] = total_reward\n",
    "                total_reward = 0\n",
    "                i += 1\n",
    "                self.policy.epsilon *= 0.99\n",
    "        \n",
    "            self.environment.reset()\n",
    "        np.save('Q_table.npy', self.Q_table)\n",
    "        self.environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting the SARSA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function SARSA take as parameters:  \n",
    "\n",
    "`environment`, that is the environment in which the agent plays,   \n",
    "\n",
    "`alpha`, that is the learning rate of the algorithm. It determines how much the agent learns from each new experience,\n",
    "\n",
    "`gamma`, that is the discount factor. Determines how much the agent cares about future rewards versus immediate rewards. \n",
    "\n",
    "`epsilon`, that is the parameter for the epsilon-greedy policy. Determine the probability that the agent will choose a random action over the action it thinks is best.  \n",
    "\n",
    "`episodes`: This is the number of episodes the agent must play while learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = SARSA(env, 0.8, 0.2, 0.3, 30000)\n",
    "sarsa.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots the sum of rewards every 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(200, len(sarsa.rewards) * 100 + 1, 100)\n",
    "z = np.polyfit(x, sarsa.rewards[1:], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x, sarsa.rewards[1:])\n",
    "plt.plot(x, p(x), \"r--\") \n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per 100 Episode')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
